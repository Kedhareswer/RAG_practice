{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "37c7da26-a40d-4503-8239-c6057bee2366",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: this is a big install (many optional connectors). Remove packages you don't need.\n",
    "#!pip install --upgrade --quiet \\\n",
    "#  langchain langchain-community langchain-huggingface \\\n",
    "#  langchain-google-genai google-generativeai \\\n",
    "#  sentence-transformers transformers accelerate huggingface-hub sentencepiece \\\n",
    "#  faiss-cpu \\\n",
    "#  easyocr python-dotenv pandas openpyxl python-docx pypdf pillow jq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f05c41cc-db01-4e28-9daa-51999ccaeb25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ✅ Install dependencies (run once)\n",
    "#!pip install -U langchain-text-splitters spacy\n",
    "\n",
    "# ✅ Download the spaCy model (run once)\n",
    "#!python -m spacy download en_core_web_sm\n",
    "\n",
    "# ✅ Correct modern import\n",
    "from langchain_text_splitters import SpacyTextSplitter\n",
    "\n",
    "# ✅ Initialize splitter with the SpaCy pipeline name\n",
    "spacy_splitter = SpacyTextSplitter(pipeline=\"en_core_web_sm\")\n",
    "\n",
    "# Optional customization\n",
    "# spacy_splitter = SpacyTextSplitter(pipeline=\"en_core_web_sm\", separator=\"\\n\\n\", chunk_size=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d63eadbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from pypdf import PdfReader\n",
    "from docx import Document\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import spacy\n",
    "import io\n",
    "import re\n",
    "import easyocr\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import torch\n",
    "import math\n",
    "import sys\n",
    "import traceback\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Dict, Any, Optional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ffb8f48c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install spacy\n",
    "#!pip install -U langchain-text-splitters spacy\n",
    "#!python -m spacy download en_core_web_sm\n",
    "#!pip install hf_xet\n",
    "#!pip install --upgrade langchain langchain-community\n",
    "#!pip install --upgrade langchain-core\n",
    "#!pip install langchain-google-genai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b343081f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Environment setup complete\n",
      "💾 Working directory: c:\\Users\\mbkhn\\Downloads\\Inspired\\RAG\n",
      "📁 Data directory: c:\\Users\\mbkhn\\Downloads\\Inspired\\RAG\\data\n",
      "⚙️ Using device: cpu\n",
      "🔑 Keys loaded: GOOGLE_API_KEY, HUGGINGFACEHUB_API_TOKEN\n",
      "📚 Embedding Model: sentence-transformers/all-MiniLM-L6-v2\n",
      "✅ Core libraries imported successfully.\n"
     ]
    }
   ],
   "source": [
    "# --- Load environment variables ---\n",
    "load_dotenv()\n",
    "\n",
    "# ✅ Create a data directory if not exists\n",
    "BASE_DIR = os.getcwd()\n",
    "DATA_DIR = os.path.join(BASE_DIR, \"data\")\n",
    "os.makedirs(DATA_DIR, exist_ok=True)\n",
    "\n",
    "# --- Core configuration ---\n",
    "CONFIG = {\n",
    "    \"device\": \"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "    \"embedding_model\": \"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "    \"chunk_min_len\": 150,  # min chars before splitting\n",
    "    \"chunk_max_len\": 1000, # adaptive splitting upper limit\n",
    "    \"overlap_ratio\": 0.15, # contextual overlap for retriever\n",
    "    \"embedding_batch_size\": 16,\n",
    "    \"retrieval_top_k\": 5,\n",
    "}\n",
    "\n",
    "# --- Check essential env keys ---\n",
    "api_keys = {\n",
    "    \"GOOGLE_API_KEY\": os.getenv(\"GOOGLE_API_KEY\"),\n",
    "    \"HUGGINGFACEHUB_API_TOKEN\": os.getenv(\"HUGGINGFACEHUB_API_TOKEN\"),\n",
    "}\n",
    "\n",
    "print(\"✅ Environment setup complete\")\n",
    "print(f\"💾 Working directory: {BASE_DIR}\")\n",
    "print(f\"📁 Data directory: {DATA_DIR}\")\n",
    "print(f\"⚙️ Using device: {CONFIG['device']}\")\n",
    "print(f\"🔑 Keys loaded: {', '.join([k for k, v in api_keys.items() if v]) or 'None'}\")\n",
    "print(f\"📚 Embedding Model: {CONFIG['embedding_model']}\")\n",
    "\n",
    "# --- Quick import test ---\n",
    "try:\n",
    "    import langchain\n",
    "    import langchain_community\n",
    "    import langchain_huggingface\n",
    "    import faiss\n",
    "    from sentence_transformers import SentenceTransformer\n",
    "    print(\"✅ Core libraries imported successfully.\")\n",
    "except Exception as e:\n",
    "    print(\"❌ Library import error:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7810fc27-8eb3-46f0-917b-3d4d04833d2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ✅ Correct modern import\n",
    "from langchain_text_splitters import SpacyTextSplitter\n",
    "\n",
    "# ✅ Initialize splitter with the SpaCy pipeline name\n",
    "spacy_splitter = SpacyTextSplitter(pipeline=\"en_core_web_sm\")\n",
    "\n",
    "# Optional customization\n",
    "# spacy_splitter = SpacyTextSplitter(pipeline=\"en_core_web_sm\", separator=\"\\n\\n\", chunk_size=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6d740198-7a8b-441e-9692-6f636c409979",
   "metadata": {},
   "outputs": [],
   "source": [
    "#del hybrid_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9895d1bc-a5bf-4d65-ba9a-b046f05795aa",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded: 20251014-174708_Top AC Repa.csv\n",
      "Loaded: ICMLA_329_Comment_Response.pdf\n",
      "Failed to load ICMLA_329_Final.docx: No module named 'docx2txt'\n",
      "Loaded: Invoice_TNB-18A84296.pdf\n",
      "✅ Hybrid chunking finished. Created 175 chunks from 107 documents.\n",
      "\n",
      "--- CHUNK 1 ---\n",
      "source: data\\20251014-174708_Top AC Repa.csv\n",
      "tokens: 31\n",
      "boundary: paragraph_end\n",
      "text preview: ﻿Company Name: Shah AIR Cool Company Name link: https://www.justdial.com/Thane/Shah-AIR-Cool-Palava-City/022PXX22-XX22-231005182331-X6T7_BZDET?trkid=&term=&ncatid=10890481&area=&search=Popular%20AC%20Repair%20&%20Services%20in%20Mumbai&mncatname=AC%20Repair%20%26%20Services&ftterm=&abd_btn=Send%20En...\n",
      "\n",
      "--- CHUNK 2 ---\n",
      "source: data\\20251014-174708_Top AC Repa.csv\n",
      "tokens: 28\n",
      "boundary: paragraph_end\n",
      "text preview: ﻿Company Name: Cool India Enterprises Company Name link: https://www.justdial.com/Mumbai/Cool-India-Enterprises-Near-Nahar-Amrit-Shakti-Andheri-East/022PXX22-XX22-110831111304-F8X9_BZDET?trkid=&term=&ncatid=10890481&area=&search=Popular%20AC%20Repair%20&%20Services%20in%20Mumbai&mncatname=AC%20Repai...\n",
      "\n",
      "--- CHUNK 3 ---\n",
      "source: data\\20251014-174708_Top AC Repa.csv\n",
      "tokens: 39\n",
      "boundary: paragraph_end\n",
      "text preview: ﻿Company Name: IT AIR CONDITIONING WORK Company Name link: https://www.justdial.com/Mumbai/IT-AIR-CONDITIONING-WORK-Behind-Shivaji-Nagar-Police-Station-Govandi-West/022PXX22-XX22-221130114245-X7R9_BZDET?trkid=&term=&ncatid=10890481&area=&search=Popular%20AC%20Repair%20&%20Services%20in%20Mumbai&mnca...\n"
     ]
    }
   ],
   "source": [
    "# ===== Cell 6 (FIXED AGAIN) — Hybrid Semantic Chunking (spaCy + transformers) with guardrails =====\n",
    "import os\n",
    "import re\n",
    "from pathlib import Path\n",
    "\n",
    "import spacy\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "# Optional: if you defined CONFIG earlier, reuse it; otherwise fall back\n",
    "try:\n",
    "    DEVICE = CONFIG.get(\"device\", \"cpu\")\n",
    "    EMBEDDING_MODEL_NAME = CONFIG.get(\"embedding_model\", \"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "except Exception:\n",
    "    DEVICE = \"cpu\"\n",
    "    EMBEDDING_MODEL_NAME = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "\n",
    "# ---------- Config / guardrails ----------\n",
    "MIN_CHARS = 150           # minimum chunk length (characters)\n",
    "MAX_CHARS = 1200          # maximum chunk length (characters)\n",
    "SIM_THRESHOLD = 0.72      # cosine similarity threshold to continue merging sentences\n",
    "EMBED_BATCH = 32          # batch size for sentence embedding\n",
    "DATA_DIR = \"data\"         # fallback data dir to load files if needed\n",
    "\n",
    "# ---------- Ensure spaCy model is available and increase max length ----------\n",
    "try:\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "except Exception as e:\n",
    "    raise RuntimeError(\n",
    "        \"spaCy model en_core_web_sm not found. Run in terminal: python -m spacy download en_core_web_sm\"\n",
    "    ) from e\n",
    "\n",
    "# Increase max length safely (just assign an int)\n",
    "nlp.max_length = 10_000_000\n",
    "\n",
    "# ---------- Embedding model (SentenceTransformers) ----------\n",
    "embed_model = SentenceTransformer(EMBEDDING_MODEL_NAME, device=DEVICE)\n",
    "\n",
    "# ---------- Utility helpers ----------\n",
    "BOILERPLATE_PATTERNS = [\n",
    "    r\"^\\s*figure\\s*\\d+\", r\"^\\s*page\\s*\\d+\", r\"^\\s*table\\s*\\d+\", r\"^\\s*copyright\", r\"^\\s*.\\s*$\"\n",
    "]\n",
    "\n",
    "def is_boilerplate(s: str) -> bool:\n",
    "    s_strip = s.strip().lower()\n",
    "    if len(s_strip) < 8:\n",
    "        return True\n",
    "    for p in BOILERPLATE_PATTERNS:\n",
    "        if re.search(p, s_strip):\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def looks_like_table(text: str) -> bool:\n",
    "    if text.count(\"|\") >= 2 or \"\\t\" in text:\n",
    "        return True\n",
    "    comma_fraction = sum(1 for tok in text.split() if re.match(r\"^[\\d,\\.%-]+$\", tok)) / max(1, len(text.split()))\n",
    "    if comma_fraction > 0.25 and len(text.split()) > 6:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "# ---------- Get documents (use existing variables if present) ----------\n",
    "# Prefer workspace variables created earlier: documents or all_docs\n",
    "if \"hybrid_chunks\" in globals():\n",
    "    raise RuntimeError(\"hybrid_chunks already exists in the workspace — delete or rename it before re-running this cell.\")\n",
    "\n",
    "docs_source = None\n",
    "if \"documents\" in globals() and isinstance(documents, list) and len(documents) > 0:\n",
    "    docs_source = documents\n",
    "elif \"all_docs\" in globals() and isinstance(all_docs, list) and len(all_docs) > 0:\n",
    "    docs_source = all_docs\n",
    "else:\n",
    "    # fallback: load from DATA_DIR using basic loaders (no OCR here)\n",
    "    from langchain_community.document_loaders import (\n",
    "        TextLoader, PyPDFLoader, Docx2txtLoader, CSVLoader, UnstructuredExcelLoader, UnstructuredPowerPointLoader\n",
    "    )\n",
    "    docs_source = []\n",
    "    for f in sorted(os.listdir(DATA_DIR)):\n",
    "        p = os.path.join(DATA_DIR, f)\n",
    "        ext = os.path.splitext(f)[1].lower()\n",
    "        try:\n",
    "            if ext == \".txt\":\n",
    "                docs_source.extend(TextLoader(p).load())\n",
    "            elif ext == \".pdf\":\n",
    "                docs_source.extend(PyPDFLoader(p).load())\n",
    "            elif ext == \".docx\":\n",
    "                docs_source.extend(Docx2txtLoader(p).load())\n",
    "            elif ext == \".csv\":\n",
    "                docs_source.extend(CSVLoader(p).load())\n",
    "            elif ext in [\".xls\", \".xlsx\"]:\n",
    "                docs_source.extend(UnstructuredExcelLoader(p).load())\n",
    "            elif ext in [\".ppt\", \".pptx\"]:\n",
    "                docs_source.extend(UnstructuredPowerPointLoader(p).load())\n",
    "            else:\n",
    "                # skip images (assume OCR handled earlier) or unsupported types\n",
    "                continue\n",
    "            print(f\"Loaded: {f}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to load {f}: {e}\")\n",
    "\n",
    "if not docs_source:\n",
    "    raise RuntimeError(\"No documents found. Place files in ./data or create 'documents' / 'all_docs' variables before running.\")\n",
    "\n",
    "# ---------- Core hybrid chunking algorithm ----------\n",
    "hybrid_chunks_out = []\n",
    "\n",
    "for doc_idx, doc in enumerate(docs_source):\n",
    "    # Accept either langchain Document or simple dict/str\n",
    "    text = doc.page_content if hasattr(doc, \"page_content\") else (doc.get(\"text\") if isinstance(doc, dict) else str(doc))\n",
    "    src_meta = (doc.metadata.get(\"source\") if hasattr(doc, \"metadata\") else None) or (getattr(doc, \"source\", None) or f\"doc_{doc_idx}\")\n",
    "    if not text or not text.strip():\n",
    "        continue\n",
    "\n",
    "    # Normalize newlines\n",
    "    text = text.replace(\"\\r\\n\", \"\\n\").replace(\"\\r\", \"\\n\")\n",
    "\n",
    "    # Heuristic paragraph splitting\n",
    "    paragraphs = [p.strip() for p in text.split(\"\\n\\n\") if p.strip()]\n",
    "\n",
    "    for para in paragraphs:\n",
    "        # If very short paragraph or looks like table -> keep whole paragraph as chunk\n",
    "        if looks_like_table(para) or len(para) < MIN_CHARS:\n",
    "            hybrid_chunks_out.append({\n",
    "                \"text\": para,\n",
    "                \"source\": src_meta,\n",
    "                \"tokens\": len(para.split()),\n",
    "                \"boundary_reason\": \"table_or_short_para\"\n",
    "            })\n",
    "            continue\n",
    "\n",
    "        # Sentence segmentation with spaCy (fallback to naive split if spaCy fails)\n",
    "        try:\n",
    "            doc_sp = nlp(para)\n",
    "            sents = [s.text.strip() for s in doc_sp.sents if s.text.strip()]\n",
    "        except Exception:\n",
    "            sents = [s.strip() for s in re.split(r'(?<=[\\.\\?\\!])\\s+', para) if s.strip()]\n",
    "\n",
    "        # Filter boilerplate\n",
    "        sents = [s for s in sents if not is_boilerplate(s)]\n",
    "        if not sents:\n",
    "            continue\n",
    "\n",
    "        # Embed sentences (vector form) in batch\n",
    "        sent_embeddings = embed_model.encode(sents, convert_to_tensor=True, batch_size=EMBED_BATCH)\n",
    "\n",
    "        # Merge neighbor sentences by semantic similarity and size constraints\n",
    "        cur_indices = []\n",
    "        cur_len = 0\n",
    "        cur_emb_sum = None\n",
    "        for i, sent in enumerate(sents):\n",
    "            sent_len = len(sent)\n",
    "            sent_emb = sent_embeddings[i]\n",
    "\n",
    "            if not cur_indices:\n",
    "                cur_indices = [i]\n",
    "                cur_len = sent_len\n",
    "                cur_emb_sum = sent_emb.clone()\n",
    "                continue\n",
    "\n",
    "            cur_mean_emb = (cur_emb_sum / len(cur_indices))\n",
    "            sim = util.cos_sim(cur_mean_emb, sent_emb).item()\n",
    "\n",
    "            # Merge if similar and size allows, or if current chunk still too short\n",
    "            if (sim >= SIM_THRESHOLD and cur_len + sent_len <= MAX_CHARS) or (cur_len < MIN_CHARS):\n",
    "                cur_indices.append(i)\n",
    "                cur_len += sent_len + 1\n",
    "                cur_emb_sum = cur_emb_sum + sent_emb\n",
    "            else:\n",
    "                # finalize current chunk\n",
    "                chunk_text = \" \".join([sents[j] for j in cur_indices]).strip()\n",
    "                if len(chunk_text) >= MIN_CHARS:\n",
    "                    hybrid_chunks_out.append({\n",
    "                        \"text\": chunk_text,\n",
    "                        \"source\": src_meta,\n",
    "                        \"tokens\": len(chunk_text.split()),\n",
    "                        \"boundary_reason\": \"semantic_split\"\n",
    "                    })\n",
    "                # start new chunk\n",
    "                cur_indices = [i]\n",
    "                cur_len = sent_len\n",
    "                cur_emb_sum = sent_emb.clone()\n",
    "\n",
    "        # finalize last chunk in paragraph\n",
    "        if cur_indices:\n",
    "            chunk_text = \" \".join([sents[j] for j in cur_indices]).strip()\n",
    "            if len(chunk_text) >= 1:\n",
    "                hybrid_chunks_out.append({\n",
    "                    \"text\": chunk_text,\n",
    "                    \"source\": src_meta,\n",
    "                    \"tokens\": len(chunk_text.split()),\n",
    "                    \"boundary_reason\": \"paragraph_end\"\n",
    "                })\n",
    "\n",
    "# ---------- Postprocess: dedupe & light filtering ----------\n",
    "final_chunks = []\n",
    "seen = set()\n",
    "for ch in hybrid_chunks_out:\n",
    "    txt = re.sub(r'\\s+', ' ', ch[\"text\"]).strip()\n",
    "    if len(txt) < 10:\n",
    "        continue\n",
    "    if txt in seen:\n",
    "        continue\n",
    "    seen.add(txt)\n",
    "    # mark short but meaningful chunks\n",
    "    if len(txt) < MIN_CHARS and not looks_like_table(txt):\n",
    "        ch[\"boundary_reason\"] = ch.get(\"boundary_reason\", \"\") + \"|short\"\n",
    "    ch[\"text\"] = txt\n",
    "    final_chunks.append(ch)\n",
    "\n",
    "# Expose to notebook namespace\n",
    "hybrid_chunks = final_chunks\n",
    "\n",
    "print(f\"✅ Hybrid chunking finished. Created {len(hybrid_chunks)} chunks from {len(docs_source)} documents.\")\n",
    "for i, ex in enumerate(hybrid_chunks[:3]):\n",
    "    print(f\"\\n--- CHUNK {i+1} ---\\nsource: {ex['source']}\\ntokens: {ex['tokens']}\\nboundary: {ex['boundary_reason']}\\ntext preview: {ex['text'][:300]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0c580972-a255-4ca0-a2c8-b866ad67b78a",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building embeddings for 175 chunks using model: sentence-transformers/all-MiniLM-L6-v2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mbkhn\\AppData\\Local\\Temp\\ipykernel_3912\\768902576.py:57: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the `langchain-huggingface package and should be used instead. To use it run `pip install -U `langchain-huggingface` and import as `from `langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embedding_model = HuggingFaceEmbeddings(model_name=EMB_MODEL)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Vectorstore saved to: C:\\Users\\mbkhn\\Downloads\\Inspired\\RAG\\vectorstore\n",
      "✅ Retriever ready. Top-k = 5\n",
      "\n",
      "Sample retrieval for: What is the phone number or address of Shah AIR Cool?\n",
      "  1. source=data\\20251014-174708_Top AC Repa.csv preview=﻿Company Name: AIR Cool Services Company Name link: https://www.justdial.com/Mumbai/AIR-Cool-Services-Behind-Wadia-School-Near-D-N-Nagar-Andheri-West/022PXX22-XX22-170616171658-Z5P2_BZDET?trkid=&term=...\n",
      "  2. source=data\\20251014-174708_Top AC Repa.csv preview=﻿Company Name: Shah Air Conditioner Company Name link: https://www.justdial.com/Kalyan/Shah-Air-Conditioner-Kalyan-East/022PXX22-XX22-250924155602-Y5B4_BZDET?trkid=&term=&ncatid=10890481&area=&search=...\n",
      "  3. source=data\\20251014-174708_Top AC Repa.csv preview=﻿Company Name: Shah AIR Cool Company Name link: https://www.justdial.com/Thane/Shah-AIR-Cool-Palava-City/022PXX22-XX22-231005182331-X6T7_BZDET?trkid=&term=&ncatid=10890481&area=&search=Popular%20AC%20...\n"
     ]
    }
   ],
   "source": [
    "# ===== Cell 7 (robust) — Build Embeddings + FAISS vector store (persisted) =====\n",
    "import os\n",
    "from pathlib import Path\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# --- safe imports for Document (works across LangChain versions) ---\n",
    "try:\n",
    "    from langchain.schema import Document\n",
    "except Exception:\n",
    "    try:\n",
    "        from langchain_core.documents import Document\n",
    "    except Exception:\n",
    "        from dataclasses import dataclass\n",
    "        @dataclass\n",
    "        class Document:\n",
    "            page_content: str\n",
    "            metadata: dict = None\n",
    "\n",
    "# --- embeddings & vectorstore imports ---\n",
    "try:\n",
    "    from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "    from langchain_community.vectorstores import FAISS\n",
    "except Exception as e:\n",
    "    raise RuntimeError(\n",
    "        \"Could not import langchain_community embeddings/vectorstores. \"\n",
    "        \"Install/update langchain-community and restart the kernel.\"\n",
    "    ) from e\n",
    "\n",
    "# Settings\n",
    "VECTOR_DIR = Path(\"vectorstore\")\n",
    "VECTOR_DIR.mkdir(exist_ok=True)\n",
    "EMB_MODEL = os.environ.get(\"EMB_MODEL\", None) or (globals().get(\"CONFIG\", {}).get(\"embedding_model\", \"sentence-transformers/all-MiniLM-L6-v2\"))\n",
    "BATCH = globals().get(\"CONFIG\", {}).get(\"embedding_batch_size\", 16)\n",
    "TOP_K = globals().get(\"CONFIG\", {}).get(\"retrieval_top_k\", 5)\n",
    "\n",
    "# Ensure hybrid_chunks exists\n",
    "if \"hybrid_chunks\" not in globals() or not isinstance(hybrid_chunks, list) or len(hybrid_chunks) == 0:\n",
    "    raise RuntimeError(\"hybrid_chunks not found or empty — run the hybrid chunking cell first.\")\n",
    "\n",
    "print(f\"Building embeddings for {len(hybrid_chunks)} chunks using model: {EMB_MODEL}\")\n",
    "\n",
    "# Convert to Document objects (with metadata)\n",
    "doc_objects = [\n",
    "    Document(\n",
    "        page_content=ch[\"text\"],\n",
    "        metadata={\n",
    "            \"source\": ch.get(\"source\"),\n",
    "            \"tokens\": ch.get(\"tokens\"),\n",
    "            \"boundary_reason\": ch.get(\"boundary_reason\"),\n",
    "            \"chunk_id\": i,\n",
    "        },\n",
    "    )\n",
    "    for i, ch in enumerate(hybrid_chunks)\n",
    "]\n",
    "\n",
    "# Initialize embedding model\n",
    "embedding_model = HuggingFaceEmbeddings(model_name=EMB_MODEL)\n",
    "\n",
    "# Build FAISS index\n",
    "vectorstore = None\n",
    "try:\n",
    "    vectorstore = FAISS.from_documents(doc_objects, embedding=embedding_model)\n",
    "except Exception as e:\n",
    "    print(\"⚠️ FAISS.from_documents failed — trying batched ingestion. Error:\", e)\n",
    "    try:\n",
    "        texts = [d.page_content for d in doc_objects]\n",
    "        metadatas = [d.metadata if d.metadata else {} for d in doc_objects]\n",
    "        vectorstore = FAISS.from_texts(texts=texts, embedding=embedding_model, metadatas=metadatas)\n",
    "    except Exception as e2:\n",
    "        raise RuntimeError(\"All FAISS creation attempts failed:\\n\" + str(e2))\n",
    "\n",
    "# Persist vectorstore\n",
    "try:\n",
    "    vectorstore.save_local(str(VECTOR_DIR))\n",
    "    print(f\"✅ Vectorstore saved to: {VECTOR_DIR.resolve()}\")\n",
    "except Exception as e:\n",
    "    print(\"⚠️ Could not persist vectorstore:\", e)\n",
    "\n",
    "# Create retriever\n",
    "try:\n",
    "    retriever = vectorstore.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": TOP_K})\n",
    "    print(\"✅ Retriever ready. Top-k =\", retriever.search_kwargs.get(\"k\"))\n",
    "except Exception as e:\n",
    "    raise RuntimeError(\"Failed to construct retriever from vectorstore: \" + str(e)) from e\n",
    "\n",
    "# Quick sanity search (robust)\n",
    "sample_q = \"What is the phone number or address of Shah AIR Cool?\"\n",
    "try:\n",
    "    # Use vectorstore.similarity_search for safety\n",
    "    hits = vectorstore.similarity_search(sample_q, k=3)\n",
    "    print(f\"\\nSample retrieval for: {sample_q}\")\n",
    "    for i, h in enumerate(hits, 1):\n",
    "        src = h.metadata.get(\"source\") if getattr(h, \"metadata\", None) else \"unknown\"\n",
    "        preview = (h.page_content[:200].replace(\"\\n\", \" \") if getattr(h, \"page_content\", None) else str(h)[:200])\n",
    "        print(f\"  {i}. source={src} preview={preview}...\")\n",
    "except Exception as e:\n",
    "    print(\"⚠️ Sample retrieval failed (non-fatal):\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cd9906e7-7e28-4bf8-890e-9460e6a6482c",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Some LangChain imports failed, using fallback implementations\n",
      "Using direct Gemini API client\n",
      "RAG system initialized successfully\n",
      "\n",
      "--- QUERY ---\n",
      "What is the phone number or address of Shah AIR Cool?\n",
      "\n",
      "--- ANSWER ---\n",
      "The phone number for Shah AIR Cool is 09035439250 and its address is Palava City Phase 2 Palava City, Thane [1].\n",
      "\n",
      "--- CONFIDENCE ---\n",
      "0.33\n",
      "\n",
      "--- SOURCES ---\n",
      "1. data\\20251014-174708_Top AC Repa.csv (score: 6.563)\n",
      "   ﻿Company Name: Shah AIR Cool Company Name link: https://www.justdial.com/Thane/Shah-AIR-Cool-Palava-City/022PXX22-XX22-231005182331-X6T7_BZDET?trkid=&term=&ncatid=10890481&area=&search=Popular%20AC%20Repair%20&%20Services%20in%20Mumbai&mncatname=AC%20Repair%20%26%20Services&ftterm=&abd_btn=Send%20En...\n",
      "\n",
      "2. data\\20251014-174708_Top AC Repa.csv (score: 2.943)\n",
      "   ﻿Company Name: Shah Air Conditioner Company Name link: https://www.justdial.com/Kalyan/Shah-Air-Conditioner-Kalyan-East/022PXX22-XX22-250924155602-Y5B4_BZDET?trkid=&term=&ncatid=10890481&area=&search=Popular%20AC%20Repair%20&%20Services%20in%20Mumbai&mncatname=AC%20Repair%20%26%20Services&ftterm=&ab...\n",
      "\n",
      "3. data\\20251014-174708_Top AC Repa.csv (score: 2.235)\n",
      "   ﻿Company Name: F L AIR Cool Service Company Name link: https://www.justdial.com/Mumbai/F-L-AIR-Cool-Service-Near-By-Nefrul-Tower-Wadala-West/022PXX22-XX22-230607112938-A3G6_BZDET?trkid=&term=&ncatid=10890481&area=&search=Popular%20AC%20Repair%20&%20Services%20in%20Mumbai&mncatname=AC%20Repair%20%26%...\n",
      "\n",
      "4. data\\20251014-174708_Top AC Repa.csv (score: 1.791)\n",
      "   ﻿Company Name: AIR Cool Services Company Name link: https://www.justdial.com/Navi-Mumbai/AIR-Cool-Services-Nearby-Vitthal-Mandir-Ghansoli/022PXX22-XX22-240117124558-L6J9_BZDET?trkid=&term=&ncatid=10890481&area=&search=Popular%20AC%20Repair%20&%20Services%20in%20Mumbai&mncatname=AC%20Repair%20%26%20S...\n",
      "\n",
      "\n",
      "--- VERIFICATION ---\n",
      "1. ✅ The phone number for Shah AIR Cool is 09035439250 and its address is Palava City Phase 2 Palava City, Thane [1].\n",
      "   Matched sources: data\\20251014-174708_Top AC Repa.csv\n"
     ]
    }
   ],
   "source": [
    "# ===== Cell 8 — Reranker + RAG answerer + verification (robust, full cell) =====\n",
    "# First, install required packages with specific versions\n",
    "#!pip install -q google-generativeai==0.3.2 langchain-google-genai==0.0.8 langchain-core==0.1.0\n",
    "#!pip install -q --upgrade protobuf  # Ensure protobuf is up to date\n",
    "\n",
    "# Import Google's Generative AI with error handling\n",
    "try:\n",
    "    import google.generativeai as genai\n",
    "    # Configure with your API key\n",
    "    if \"GOOGLE_API_KEY\" not in os.environ:\n",
    "        raise ValueError(\"GOOGLE_API_KEY environment variable not set\")\n",
    "    genai.configure(api_key=os.environ[\"GOOGLE_API_KEY\"])\n",
    "except ImportError:\n",
    "    raise ImportError(\"Failed to import google.generativeai. Install with: pip install google-generativeai\")\n",
    "\n",
    "# ---------------- Robust imports for PromptTemplate + LLMChain ----------------\n",
    "try:\n",
    "    from langchain.prompts import PromptTemplate\n",
    "    from langchain.chains import LLMChain\n",
    "    from langchain.schema import Document\n",
    "    from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "    LANGCHAIN_AVAILABLE = True\n",
    "except ImportError:\n",
    "    print(\"Warning: Some LangChain imports failed, using fallback implementations\")\n",
    "    LANGCHAIN_AVAILABLE = False\n",
    "\n",
    "# Fallback implementations if imports fail\n",
    "if not LANGCHAIN_AVAILABLE:\n",
    "    class Document:\n",
    "        def __init__(self, page_content: str, metadata: dict = None):\n",
    "            self.page_content = page_content\n",
    "            self.metadata = metadata or {}\n",
    "\n",
    "    class PromptTemplate:\n",
    "        def __init__(self, template: str, input_variables: List[str]):\n",
    "            self.template = template\n",
    "            self.input_variables = input_variables\n",
    "        \n",
    "        def format(self, **kwargs) -> str:\n",
    "            return self.template.format(**kwargs)\n",
    "        \n",
    "        @classmethod\n",
    "        def from_template(cls, template: str):\n",
    "            # Extract input variables from template\n",
    "            input_vars = re.findall(r'\\{([^}]+)\\}', template)\n",
    "            return cls(template=template, input_variables=list(set(input_vars)))\n",
    "\n",
    "    class LLMChain:\n",
    "        def __init__(self, llm, prompt):\n",
    "            self.llm = llm\n",
    "            self.prompt = prompt\n",
    "        \n",
    "        def run(self, inputs: dict) -> str:\n",
    "            prompt_text = self.prompt.format(**inputs)\n",
    "            return self.llm(prompt_text)\n",
    "\n",
    "# ---------------- CrossEncoder (reranker) ----------------\n",
    "try:\n",
    "    from sentence_transformers import CrossEncoder\n",
    "except ImportError:\n",
    "    raise ImportError(\n",
    "        \"Missing package `sentence-transformers`. Install it with:\\n\"\n",
    "        \"    pip install -U sentence-transformers\\n\"\n",
    "        \"Then restart the kernel.\"\n",
    "    )\n",
    "\n",
    "# ---------------- LLM Initialization ----------------\n",
    "class GeminiLLM:\n",
    "    \"\"\"Wrapper for Google's Gemini model with a consistent interface\"\"\"\n",
    "    def __init__(self, model_name: str = \"gemini-2.5-flash\", temperature: float = 0.4, max_output_tokens: int = 512):\n",
    "        self.model_name = model_name\n",
    "        self.temperature = temperature\n",
    "        self.max_output_tokens = max_output_tokens\n",
    "        self.model = genai.GenerativeModel(model_name)\n",
    "    \n",
    "    def __call__(self, prompt: str) -> str:\n",
    "        try:\n",
    "            response = self.model.generate_content(\n",
    "                prompt,\n",
    "                generation_config=genai.types.GenerationConfig(\n",
    "                    temperature=self.temperature,\n",
    "                    max_output_tokens=self.max_output_tokens\n",
    "                )\n",
    "            )\n",
    "            return response.text\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"Error calling Gemini API: {str(e)}\")\n",
    "\n",
    "# Initialize LLM\n",
    "try:\n",
    "    if LANGCHAIN_AVAILABLE:\n",
    "        llm = ChatGoogleGenerativeAI(\n",
    "            model=\"gemini-2.5-flash\",  # Using the stable model\n",
    "            temperature=0.4,\n",
    "            max_output_tokens=512\n",
    "        )\n",
    "        print(\"Using LangChain's ChatGoogleGenerativeAI\")\n",
    "    else:\n",
    "        llm = GeminiLLM(model_name=\"gemini-2.5-flash\", temperature=0.4, max_output_tokens=512)\n",
    "        print(\"Using direct Gemini API client\")\n",
    "except Exception as e:\n",
    "    raise RuntimeError(f\"Failed to initialize LLM: {str(e)}\")\n",
    "\n",
    "# ---------------- Core RAG Components ----------------\n",
    "def _extract_text_from_output(out):\n",
    "    \"\"\"Extract text from various LLM output formats\"\"\"\n",
    "    if out is None:\n",
    "        return \"\"\n",
    "    if isinstance(out, str):\n",
    "        return out\n",
    "    # Handle direct Gemini response\n",
    "    if hasattr(out, 'text'):\n",
    "        return out.text\n",
    "    # Handle dict responses\n",
    "    if isinstance(out, dict):\n",
    "        if 'answer' in out:\n",
    "            return out['answer']\n",
    "        if 'text' in out:\n",
    "            return out['text']\n",
    "        if 'content' in out:\n",
    "            return out['content']\n",
    "    # Handle LangChain message objects\n",
    "    if hasattr(out, 'content'):\n",
    "        return out.content if isinstance(out.content, str) else str(out.content)\n",
    "    # Fallback to string representation\n",
    "    return str(out)\n",
    "\n",
    "def _call_llm_and_extract(llm_obj, prompt_text: str) -> str:\n",
    "    \"\"\"Robust LLM caller that handles different LLM interfaces\"\"\"\n",
    "    try:\n",
    "        # Try direct call first\n",
    "        if callable(llm_obj):\n",
    "            return _extract_text_from_output(llm_obj(prompt_text))\n",
    "        \n",
    "        # Try common LLM interfaces\n",
    "        for method in ['invoke', 'run', 'generate', '__call__', 'predict', 'chat']:\n",
    "            if hasattr(llm_obj, method):\n",
    "                try:\n",
    "                    result = getattr(llm_obj, method)(prompt_text)\n",
    "                    return _extract_text_from_output(result)\n",
    "                except Exception:\n",
    "                    continue\n",
    "        \n",
    "        raise RuntimeError(\"No supported call interface found on the LLM object\")\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"Error calling LLM: {str(e)}\")\n",
    "\n",
    "# ---------------- RAG Implementation ----------------\n",
    "class RAGSystem:\n",
    "    def __init__(self, llm, rerank_model: str = \"cross-encoder/ms-marco-MiniLM-L-6-v2\"):\n",
    "        self.llm = llm\n",
    "        self.reranker = CrossEncoder(rerank_model, device=\"cpu\")\n",
    "        self.prompt_template = self._create_prompt_template()\n",
    "        self.chain = self._create_chain()\n",
    "        \n",
    "    def _create_prompt_template(self):\n",
    "        template = \"\"\"\n",
    "        You are an expert assistant. Answer the question using ONLY the numbered context passages below.\n",
    "        Cite facts inline using the passage numbers like [1], [2]. If the answer cannot be found in the passages, \n",
    "        respond exactly: \"Information not found in the provided documents.\"\n",
    "\n",
    "        Question: {question}\n",
    "\n",
    "        Passages:\n",
    "        {context}\n",
    "\n",
    "        Answer (concise; preserve any formatting needed):\n",
    "        \"\"\"\n",
    "        return PromptTemplate.from_template(template)\n",
    "    \n",
    "    def _create_chain(self):\n",
    "        return LLMChain(llm=self.llm, prompt=self.prompt_template)\n",
    "    \n",
    "    def fetch_and_rerank(self, query: str, top_k: int = 20):\n",
    "        \"\"\"Fetch and rerank documents based on the query\"\"\"\n",
    "        try:\n",
    "            candidates = retriever.get_relevant_documents(query)\n",
    "        except Exception:\n",
    "            try:\n",
    "                candidates = vectorstore.similarity_search(query, k=top_k)\n",
    "            except Exception:\n",
    "                raise RuntimeError(\n",
    "                    \"Neither `retriever` nor `vectorstore` is available/working in the notebook environment.\"\n",
    "                )\n",
    "        \n",
    "        if not candidates:\n",
    "            return []\n",
    "            \n",
    "        # Prepare pairs for reranking\n",
    "        pairs = [(query, doc.page_content[:1000] + (\"...\" if len(doc.page_content) > 1000 else \"\")) \n",
    "                for doc in candidates[:top_k]]\n",
    "        \n",
    "        # Get scores from reranker\n",
    "        try:\n",
    "            scores = self.reranker.predict(pairs)\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"Reranker predict failed: {str(e)}\")\n",
    "        \n",
    "        # Sort by score\n",
    "        scored_sorted = sorted(\n",
    "            [{\"doc\": d, \"score\": float(s)} for d, s in zip(candidates, scores)],\n",
    "            key=lambda x: x[\"score\"], \n",
    "            reverse=True\n",
    "        )\n",
    "        return scored_sorted\n",
    "    \n",
    "    def build_context(self, scored_list, max_docs: int = 4):\n",
    "        \"\"\"Build context from scored documents\"\"\"\n",
    "        context_blocks, sources = [], []\n",
    "        for i, item in enumerate(scored_list[:max_docs], 1):\n",
    "            doc = item[\"doc\"]\n",
    "            text = doc.page_content if hasattr(doc, 'page_content') else str(doc)\n",
    "            text = text[:1000] + (\"...\" if len(text) > 1000 else \"\")\n",
    "            context_blocks.append(f\"[{i}] {text}\")\n",
    "            \n",
    "            metadata = getattr(doc, 'metadata', {}) or {}\n",
    "            sources.append({\n",
    "                \"id\": i,\n",
    "                \"source\": metadata.get(\"source\", \"unknown\"),\n",
    "                \"score\": item[\"score\"],\n",
    "                \"snippet\": text[:300].replace(\"\\n\", \" \")\n",
    "            })\n",
    "        \n",
    "        return \"\\n\\n\".join(context_blocks), sources\n",
    "    \n",
    "    def verify_answer(self, answer_text: str, top_k: int = 5):\n",
    "        \"\"\"Verify the facts in the answer against retrieved documents\"\"\"\n",
    "        sentences = [s.strip() for s in re.split(r'(?<=[.!?])\\s+', answer_text) if s.strip()]\n",
    "        verification = []\n",
    "        \n",
    "        for sent in sentences:\n",
    "            try:\n",
    "                try:\n",
    "                    sup_docs = retriever.get_relevant_documents(sent)[:top_k]\n",
    "                except Exception:\n",
    "                    sup_docs = vectorstore.similarity_search(sent, k=top_k)\n",
    "                \n",
    "                supported = False\n",
    "                matched_sources = set()\n",
    "                sent_words = set(re.findall(r'\\w+', sent.lower()))\n",
    "                \n",
    "                for doc in sup_docs:\n",
    "                    doc_text = doc.page_content if hasattr(doc, 'page_content') else str(doc)\n",
    "                    doc_words = set(re.findall(r'\\w+', doc_text.lower()))\n",
    "                    \n",
    "                    if len(sent_words & doc_words) >= max(3, len(sent_words) // 6):\n",
    "                        supported = True\n",
    "                        metadata = getattr(doc, 'metadata', {}) or {}\n",
    "                        if 'source' in metadata:\n",
    "                            matched_sources.add(metadata['source'])\n",
    "                \n",
    "                verification.append({\n",
    "                    \"sentence\": sent,\n",
    "                    \"supported\": supported,\n",
    "                    \"matched_sources\": list(matched_sources)\n",
    "                })\n",
    "                \n",
    "            except Exception as e:\n",
    "                verification.append({\n",
    "                    \"sentence\": sent,\n",
    "                    \"supported\": False,\n",
    "                    \"error\": str(e)\n",
    "                })\n",
    "        \n",
    "        return verification\n",
    "    \n",
    "    def answer_query(self, query: str, rerank_top_k: int = 20, final_k: int = 4):\n",
    "        \"\"\"Answer a query using the RAG system\"\"\"\n",
    "        try:\n",
    "            # Step 1: Fetch and rerank documents\n",
    "            scored = self.fetch_and_rerank(query, top_k=rerank_top_k)\n",
    "            if not scored:\n",
    "                return {\n",
    "                    \"answer\": \"No relevant documents found.\",\n",
    "                    \"sources\": [],\n",
    "                    \"confidence\": 0.0,\n",
    "                    \"verification\": []\n",
    "                }\n",
    "            \n",
    "            # Step 2: Build context\n",
    "            context, sources = self.build_context(scored, max_docs=final_k)\n",
    "            \n",
    "            # Calculate confidence based on scores\n",
    "            scores = [s[\"score\"] for s in scored[:final_k]]\n",
    "            if len(scores) > 1:\n",
    "                min_s, max_s = min(scores), max(scores)\n",
    "                confidence = ((sum(scores) / len(scores)) - min_s) / (max_s - min_s) if max_s > min_s else 1.0\n",
    "            else:\n",
    "                confidence = 1.0\n",
    "            confidence = max(0.0, min(1.0, confidence))\n",
    "            \n",
    "            # Step 3: Generate answer\n",
    "            try:\n",
    "                if hasattr(self.chain, 'run'):\n",
    "                    generated = self.chain.run({\"question\": query, \"context\": context})\n",
    "                else:\n",
    "                    prompt = self.prompt_template.format(question=query, context=context)\n",
    "                    generated = _call_llm_and_extract(self.llm, prompt)\n",
    "                \n",
    "                # Step 4: Verify the answer\n",
    "                verification = self.verify_answer(generated)\n",
    "                \n",
    "                return {\n",
    "                    \"query\": query,\n",
    "                    \"answer\": generated.strip(),\n",
    "                    \"confidence\": round(confidence, 3),\n",
    "                    \"sources\": sources,\n",
    "                    \"verification\": verification\n",
    "                }\n",
    "                \n",
    "            except Exception as e:\n",
    "                return {\n",
    "                    \"answer\": f\"Error generating answer: {str(e)}\",\n",
    "                    \"sources\": sources,\n",
    "                    \"confidence\": 0.0,\n",
    "                    \"verification\": []\n",
    "                }\n",
    "                \n",
    "        except Exception as e:\n",
    "            return {\n",
    "                \"answer\": f\"Error processing query: {str(e)}\",\n",
    "                \"sources\": [],\n",
    "                \"confidence\": 0.0,\n",
    "                \"verification\": []\n",
    "            }\n",
    "\n",
    "# Initialize the RAG system\n",
    "try:\n",
    "    rag = RAGSystem(llm=llm)\n",
    "    print(\"RAG system initialized successfully\")\n",
    "except Exception as e:\n",
    "    print(f\"Error initializing RAG system: {str(e)}\")\n",
    "    raise\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Example query\n",
    "    query = \"What is the phone number or address of Shah AIR Cool?\"\n",
    "    \n",
    "    # Get the answer\n",
    "    result = rag.answer_query(query)\n",
    "    \n",
    "    # Print the results\n",
    "    print(\"\\n--- QUERY ---\")\n",
    "    print(query)\n",
    "    \n",
    "    print(\"\\n--- ANSWER ---\")\n",
    "    print(result[\"answer\"])\n",
    "    \n",
    "    print(\"\\n--- CONFIDENCE ---\")\n",
    "    print(f\"{result['confidence']:.2f}\")\n",
    "    \n",
    "    if result[\"sources\"]:\n",
    "        print(\"\\n--- SOURCES ---\")\n",
    "        for i, source in enumerate(result[\"sources\"], 1):\n",
    "            print(f\"{i}. {source['source']} (score: {source['score']:.3f})\")\n",
    "            print(f\"   {source['snippet']}...\\n\")\n",
    "    \n",
    "    if result[\"verification\"]:\n",
    "        print(\"\\n--- VERIFICATION ---\")\n",
    "        for i, verif in enumerate(result[\"verification\"], 1):\n",
    "            status = \"✅\" if verif[\"supported\"] else \"❌\"\n",
    "            print(f\"{i}. {status} {verif['sentence'][:150]}\")\n",
    "            if verif.get(\"matched_sources\"):\n",
    "                print(f\"   Matched sources: {', '.join(verif['matched_sources'])}\")\n",
    "            if 'error' in verif:\n",
    "                print(f\"   Error: {verif['error']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "40142025-e10e-4d1b-894d-4be730aa57fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install --upgrade langchain-google-genai==0.0.8 google-generativeai==0.3.2 langchain-core==0.1.0"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
