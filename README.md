# RAG Q&A Application Suite

A comprehensive suite of Retrieval Augmented Generation (RAG) applications that have evolved from research prototypes to production-ready solutions. This project demonstrates the complete lifecycle of building AI-powered document Q&A systems using different technologies and frameworks.

## ğŸ¯ Project Overview

This repository contains multiple implementations of a RAG-based Q&A system that allows users to upload documents and ask questions about their content. The system uses local embeddings for privacy and Google's Gemini API for natural language responses.

### Key Features
- ğŸ“„ **Multi-format Document Support**: PDF, DOCX, TXT, CSV, Excel, JSON, Markdown, Images (with OCR)
- ğŸ§  **Local Embeddings**: Client-side processing using Transformer.js
- ğŸ’¬ **Conversational AI**: Powered by Google Gemini 2.0 Flash
- ğŸ”„ **Persistent Storage**: Documents and conversations saved between sessions
- ğŸ¨ **Modern UI**: Clean, responsive interface with dark theme
- ğŸ“± **Cross-platform**: Works on desktop and mobile devices

## ğŸ—ï¸ Architecture Evolution

This project showcases the evolution of a RAG system through different implementation stages:

### Phase 1: Research Prototype (Jupyter Notebook)
- **Location**: `jupyter_notebook/RAG.ipynb`
- **Tech Stack**: Python, LangChain, FAISS, HuggingFace Embeddings
- **Purpose**: Initial research and experimentation
- **Features**: Basic document loading, chunking, and vector search

### Phase 2: Web Prototype (Streamlit)
- **Location**: `streamlite/`
- **Tech Stack**: Streamlit, LangChain, FAISS, EasyOCR
- **Purpose**: First web interface for document Q&A
- **Features**: Drag-and-drop uploads, real-time chat interface

### Phase 3: Production App (Next.js)
- **Location**: `Application/`
- **Tech Stack**: Next.js 14, TypeScript, Transformer.js, Tailwind CSS
- **Purpose**: Production-ready, scalable web application
- **Features**: Advanced UI, persistent storage, optimized performance

## ğŸš€ Current Implementation (Next.js App)

The main application is built with modern web technologies and provides the most advanced features.

### Tech Stack

#### Frontend
- **Framework**: Next.js 14 (App Router)
- **Language**: TypeScript
- **Styling**: Tailwind CSS + Shadcn UI
- **Icons**: Lucide React

#### AI & ML
- **Embeddings**: Transformer.js with `all-MiniLM-L6-v2`
- **LLM**: Google Gemini 2.0 Flash API
- **Vector Search**: Cosine similarity with in-memory storage

#### Document Processing
- **PDF**: pdf-parse
- **DOCX**: mammoth
- **Excel/CSV**: xlsx
- **Images**: tesseract.js (OCR)
- **JSON/Markdown**: Native parsing

#### Storage
- **Vectors**: File system persistence (`.cache/vector-store.json`)
- **State**: Browser localStorage
- **Session**: Automatic state management

## ğŸ“ Project Structure

```
RAG/
â”œâ”€â”€ Application/                    # Main Next.js application
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ app/                    # Next.js app router
â”‚   â”‚   â”‚   â”œâ”€â”€ api/                # API endpoints
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ upload/         # Document processing
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ query/          # RAG query handling
â”‚   â”‚   â”‚   â”‚   â””â”€â”€ vector-store/   # Storage management
â”‚   â”‚   â”‚   â”œâ”€â”€ globals.css         # Global styles
â”‚   â”‚   â”‚   â”œâ”€â”€ layout.tsx          # Root layout
â”‚   â”‚   â”‚   â””â”€â”€ page.tsx            # Main application page
â”‚   â”‚   â”œâ”€â”€ components/             # React components
â”‚   â”‚   â”‚   â”œâ”€â”€ ui/                 # Reusable UI components
â”‚   â”‚   â”‚   â”œâ”€â”€ FileUpload.tsx      # File upload interface
â”‚   â”‚   â”‚   â”œâ”€â”€ ChatInterface.tsx   # Chat UI components
â”‚   â”‚   â”‚   â”œâ”€â”€ LeftSidebar.tsx     # Document management
â”‚   â”‚   â”‚   â”œâ”€â”€ RightSidebar.tsx    # Settings panel
â”‚   â”‚   â”‚   â””â”€â”€ ChatGPTInterface.tsx # Main chat interface
â”‚   â”‚   â”œâ”€â”€ lib/                    # Core business logic
â”‚   â”‚   â”‚   â”œâ”€â”€ documentLoader.ts   # Document processing
â”‚   â”‚   â”‚   â”œâ”€â”€ embeddings.ts       # Text embeddings
â”‚   â”‚   â”‚   â”œâ”€â”€ vectorStore.ts      # Vector database
â”‚   â”‚   â”‚   â””â”€â”€ gemini.ts           # LLM integration
â”‚   â”‚   â””â”€â”€ types/                  # TypeScript definitions
â”‚   â”œâ”€â”€ public/                     # Static assets
â”‚   â”œâ”€â”€ .cache/                     # Persistent vector storage
â”‚   â”œâ”€â”€ package.json                # Dependencies
â”‚   â””â”€â”€ README.md                   # Application documentation
â”œâ”€â”€ streamlite/                     # Streamlit prototype
â”‚   â”œâ”€â”€ streamlit_app.py            # Main application
â”‚   â””â”€â”€ requirements.txt            # Python dependencies
â”œâ”€â”€ jupyter_notebook/               # Research prototype
â”‚   â””â”€â”€ RAG.ipynb                   # Jupyter notebook
â”œâ”€â”€ input/                          # Sample documents
â”‚   â”œâ”€â”€ outline_data.json          # Academic project outline
â”‚   â”œâ”€â”€ AI and Plag Report_chunks.md # Research content
â”‚   â”œâ”€â”€ ICMLA_329_Comment_Response.pdf # Academic paper
â”‚   â””â”€â”€ ...                        # Additional sample files
â”œâ”€â”€ .env                           # Environment variables
â””â”€â”€ README.md                      # This file
```

## ğŸ› ï¸ Installation & Setup

### Prerequisites
- Node.js 18+ (LTS recommended)
- Google Gemini API key ([Get one here](https://makersuite.google.com/app/apikey))
- npm or yarn package manager

### Quick Start

1. **Clone and navigate to the Application directory:**
   ```bash
   cd Application
   ```

2. **Install dependencies:**
   ```bash
   npm install
   ```

3. **Set up environment variables:**
   - Copy `.env.local.example` to `.env.local` (if it exists)
   - Or add your Google API key directly in the app UI

4. **Start the development server:**
   ```bash
   npm run dev
   ```

5. **Open [http://localhost:3000](http://localhost:3000)** in your browser

### Alternative: Run Streamlit Version

1. **Navigate to streamlite directory:**
   ```bash
   cd streamlite
   ```

2. **Install Python dependencies:**
   ```bash
   pip install -r requirements.txt
   ```

3. **Run the Streamlit app:**
   ```bash
   streamlit run streamlit_app.py
   ```

## ğŸ“– Usage Guide

### Getting Started
1. **Enter API Key**: Paste your Google Gemini API key in the top input field
2. **Upload Documents**: Click "Upload Documents" or drag files to upload
3. **Ask Questions**: Type your questions in the chat input and press Enter
4. **View Sources**: Check the sources panel for document citations

### Supported File Types
- **Documents**: PDF, DOCX, TXT, MD
- **Data Files**: CSV, XLSX, JSON
- **Images**: JPG, PNG (with OCR text extraction)

### Features
- **Persistent Storage**: Documents remain available between sessions
- **Multiple Chat Sessions**: Create and manage separate conversations
- **Settings Panel**: Adjust AI parameters (temperature, model, etc.)
- **Source Citations**: View which documents contain relevant information
- **Responsive Design**: Works on all screen sizes

## ğŸ“Š Sample Data

The `input/` directory contains sample documents to test the system:

- **Academic Content**: Capstone project reports, research papers
- **Data Files**: CSV datasets, JSON configurations
- **Images**: Screenshots and diagrams (processed via OCR)
- **Mixed Formats**: Demonstrates multi-format document processing

### Example Queries
- "What is the main objective of this capstone project?"
- "Summarize the research methodology used"
- "What datasets were used in the experiments?"
- "What are the key findings from the results?"

## ğŸ”§ Configuration

### Environment Variables
```bash
GOOGLE_API_KEY=your_gemini_api_key_here
```

### Application Settings
- **Model**: Choose between different Gemini models
- **Temperature**: Control response creativity (0.0 - 1.0)
- **Chunk Size**: Text chunk size for processing (default: 1000)
- **Overlap**: Chunk overlap for context preservation (default: 200)
- **Top-K**: Number of similar chunks to retrieve (default: 4)

## ğŸ§ª Development & Testing

### Running Tests
```bash
cd Application
npm run lint
npm run build
```

### Development Commands
```bash
npm run dev      # Start development server
npm run build    # Build for production
npm run start    # Start production server
npm run lint     # Run ESLint
```

## ğŸ¤ Contributing

Contributions are welcome! This project demonstrates the evolution of RAG systems and can benefit from:

- Additional document format support
- Performance optimizations
- UI/UX improvements
- New AI model integrations
- Testing and documentation improvements

### Development Setup
1. Fork the repository
2. Create a feature branch
3. Make your changes
4. Test thoroughly
5. Submit a pull request

## ğŸ“ˆ Performance & Limitations

### Strengths
- **Privacy**: Local embeddings, no data sent to external servers
- **Speed**: Optimized for real-time responses
- **Accuracy**: Context-aware responses with source citations
- **Flexibility**: Multiple implementation approaches

### Current Limitations
- **Memory**: Large document sets may impact performance
- **Session-based**: Vector store resets on server restart
- **Single-user**: Not designed for concurrent multi-user access
- **API Dependency**: Requires Google Gemini API key

### Future Enhancements
- Persistent vector database (PostgreSQL + pgvector)
- Multi-user support with authentication
- Streaming responses for better UX
- Advanced filtering and search capabilities

## ğŸ“š Technical Documentation

### Core Components

#### Document Processing Pipeline
1. **Upload**: Files received via multipart form data
2. **Processing**: Format-specific parsers extract text
3. **Chunking**: RecursiveCharacterTextSplitter creates overlapping chunks
4. **Embedding**: Transformer.js generates vector representations
5. **Storage**: Vectors saved to persistent file storage

#### Query Processing Pipeline
1. **Input**: Natural language query received
2. **Embedding**: Query converted to vector representation
3. **Retrieval**: Cosine similarity search finds relevant chunks
4. **Generation**: Gemini API generates contextual response
5. **Response**: Answer with sources returned to user

### API Endpoints

#### `/api/upload`
- **Method**: POST
- **Purpose**: Process and store uploaded documents
- **Input**: FormData with files
- **Output**: Processing statistics

#### `/api/query`
- **Method**: POST
- **Purpose**: Handle RAG queries
- **Input**: Query text, API key, settings
- **Output**: AI response with sources and metrics

#### `/api/vector-store`
- **Method**: GET/POST
- **Purpose**: Manage vector storage
- **Features**: Status checking, data clearing

## ğŸ”— Related Resources

- [Transformer.js Documentation](https://huggingface.co/docs/transformers.js)
- [Google Gemini API Docs](https://ai.google.dev/docs)
- [Next.js Documentation](https://nextjs.org/docs)
- [LangChain Documentation](https://python.langchain.com/)

## ğŸ“„ License

This project is licensed under the MIT License - see the LICENSE file for details.

## ğŸ™ Acknowledgments

- **Transformer.js** team for local embedding capabilities
- **Google AI** for Gemini API access
- **Next.js** team for the excellent framework
- **Hugging Face** for the embedding model
- **LangChain** community for initial inspiration

---

*Built with â¤ï¸ using modern web technologies and AI*
