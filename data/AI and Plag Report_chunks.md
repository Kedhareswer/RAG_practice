# Document Chunks: AI and Plag Report.pdf

Exported: 12/10/2025, 8:32:48 pm
Total Chunks: 51

---

## Chunk 1

=== Page 1 ===
Scan details Scan Time Total Pages Total Words July 15th, 2025 at 11:12 UTC 16 3885 Analysis Report Plagiarism Detection and AI Detection Report I CMLA_329_Final (1).docx Plagiarism Detection AI Detection 3.2% 0% Plagiarism Types Text Coverage Words Text Coverage Words I dentical 0.6% 25 A I Text 0% 0 Minor Changes 0.4% 14 Human Text 100% 3,876 Paraphrased 2.2% 84 Excluded Omitted Words 9 Excluded Omitted Words 9 C e r t i f i e d b y About this report help.copyleaks.com

---

## Chunk 2

=== Page 2 ===
Plagiarism 3. 2% Results (8) *Results may not appear because the feature has been disabled.  Plagiarism Types Text Coverage Words I dentical 0. 6% 25 Private Cloud Hub Shared Data Hub Filtered / Excluded Minor Changes 0. 4% 14 0 0 0 Paraphrased 2. 2% 84 Excluded Internet Sources AI Source Match Current Batch 8 0 0 Omitted Words 9 About Plagiarism Detection Our A I -powered plagiarism scans offer three layers of text similarity detection: I dentical, Minor Changes, and Paraphrased.  Based on your scan settings we also provide insight on how much of the text you are not scanning for plagiarism (Omitted words).  Identical Minor Changes One to one exact word matches.  Learn more Words that hold nearly the same meaning but have a change to their form (e. g. “large” becomes “largely”).  Learn more Paraphrased Omitted Words Different words that hold the same meaning that replace the original content (e. g.

---

## Chunk 3

Learn more Paraphrased Omitted Words Different words that hold the same meaning that replace the original content (e.  g. The portion of text that is not being scanned for plagiarism based on the scan settings.  "large" becomes "big") Learn more (e. g.  the ' I gnore quotations' setting is enabled and the document is 20% quotations making the omitted words percentage 20%) Learn more Copyleaks Shared Data Hub Our Shared Data Hub is a collection of millions of user-submitted documents that you can utilize as a scan resource and choose whether or not you would like to submit the file you are scanning into the Shared Data Hub.  Learn more Filtered and Excluded Results The report will generate a complete list of results.  There is always the option to exclude specific results that are not relevant.  Note, by unchecking certain results, the similarity percentage may change.

---

## Chunk 4

There is always the option to exclude specific results that are not relevant.   Note, by unchecking certain results, the similarity percentage may change. Learn more Current Batch Results These are the results displayed from the collection, or batch, of files uploaded for a scan at the same time.

---

## Chunk 5

=== Page 3 ===
Plagiarism Detection Results: (8) 2024. 12. 17. 628939v1. full. pdf 1. 3% https:/ /www. biorxiv. org/content/10. 1101/2024. 12. 17. 628939v1. full. pdf bioRxiv preprint doi: https:/ /doi. org/10. 1101/2024. 12. 17. 628939; this version posted December 21, 2024.  The copyright holder for this pre...  Oracle bone inscription image restoration via glyph extraction | npj Heritage...  1% https:/ /www. nature. com/articles/s40494-025-01795-8 Skip to main content Thank you for visiting nature. com.  You are using a browser version with limited s...  (PDF) Enhancing the Super-Resolution of Medical I mages: I ntroducing the Deep ...  1% https:/ /www. researchgate. net/publication/375782191_enhancing_the_super-resolution_of_medical_images_intro … ArticlePDF AvailableEnhancing the Super-Resolution of Medical I mages: I ntroducing the Deep Residual Feature Distillation Channel...  Lightweight Super-Resolution Techniques in Medical I maging: Bridging Quality ...  0. 7% https:/ /pmc.

---

## Chunk 6

0.  7% https:/ /pmc.ncbi. nlm. nih. gov/articles/pmc11673497/ Skip to main content ...  An efficient medical data encryption scheme using selective shuffling and int...  0. 7% https:/ /www. nature. com/articles/s41598-025-85539-5 Skip to main content Thank you for visiting nature. com.  You are using a browser version with limited s...  ScHiCAtt: Enhancing single-cell Hi-C data resolution using attention-based mo...  0. 3% https:/ /pmc. ncbi. nlm. nih. gov/articles/pmc11953966/ Skip to main content ...  Transforming H&E images into I HC: A Variance-Penalized GAN for Precision Onco...  0. 3% https:/ /arxiv. org/html/2506. 18371 I I ntroduction II Literature Review III Proposed Methodology III -A Pyramid pix2pix Framework III -B Variance-Based Loss for Mode ...  I JRT I 0. 3% https:/ /www. ijcrt. org/papers/ijcrt2504146. pdf Hitesh www. ijcrt. org © 2025 I JCRT | Volume 13, I ssue 4 April 2025 | I SSN: 2320-2882 Elevating I mage Resolution Using Deep And Shallow Network ...

---

## Chunk 7

=== Page 4 ===
Enhancement of Endoscopic Images and Video Using Advanced Deep Learning and Traditional Image Enhancement Techniques 1 [0009-0007-4095-5329] 2 [0009-0003-7635-0592] Jagadesh Chilla , M.  Kedhareswer Naidu 3 [0009-0006-4203-6564] 4 [0009-0006-8143-9104] 5 Manish Chetla , Aman Kumar , Usha [0000-0001-6143-1312] 6 [0000-0003-3102-0808] Mittal , Shamneesh Sharma 1,2,3,5 School of Computer Science and Engineering, Lovely Professional University, India 4,6 upGrad Education Private Limited, Punjab, India Abstract.  Endoscopic images often struggle with issues like low contrast, noise, and lack of detail, making it harder for doctors to make accurate diagnoses.  To help overcome these problems, our team developed a new image enhancement method using deep learning techniques.  The system combines a Dual-Input U-Net with a Super-Resolution Convolutional Neural Network (SRCNN).  The SRCNN takes a low-quality, low-resolution image and produces a clearer, higher-resolution version.

---

## Chunk 8

The system combines a Dual-Input U-Net with a Super-Resolution Convolutional Neural Network (SRCNN).   The SRCNN takes a low-quality, low-resolution image and produces a clearer, higher-resolution version. Both this improved image and the original are then processed together by the Dual-Input U-Net for further refinement .  After that, we apply several post-processing steps such as removing noise, adjusting brightness and contrast, fine-tuning lighting, and sharpening to make the images even clearer.  We test how well this system works using metrics like Peak Signal-to-Noise Ratio (PSNR) and Structural Similarity Index Measure which show our method outperforms traditional techniques.  Overall, (SSIM), our results suggest that this approach can greatly improve endoscopic images, potentially helping doctors make more accurate diagnoses.  Keywords: Endoscopic Imaging, SRCNN, Dual-Input U-Net, Deep Learning, Image Enhancement, Medical Imaging, Post-Processing, SSIM, PSNR.

---

## Chunk 9

Overall, (SSIM), our results suggest that this approach can greatly improve endoscopic images, potentially helping doctors make more accurate diagnoses.   Keywords: Endoscopic Imaging, SRCNN, Dual-Input U-Net, Deep Learning, Image Enhancement, Medical Imaging, Post-Processing, SSIM, PSNR. 1 Introduction Endoscopic imaging has really become an essential part of modern medical diagnostics, especially when it comes to checking out the gastrointestinal tract, lungs, and other internal organs.  That said, even though it's widely used, t raditional endoscopic images often leave something t o be desired they can be blurry, noisy, or have poor contrast.  These issues usually happen because of limitations in the camera hardware, not-so-great lighting conditions, or slight movements during the procedure, which can lead to blurry or di storted pictures [1, 2].

---

## Chunk 10

That said, even though it's widely used, t raditional endoscopic images often leave something t o be desired they can be blurry, noisy, or have poor contrast.   These issues usually happen because of limitations in the camera hardware, not-so-great lighting conditions, or slight movements during the procedure, which can lead to blurry or di storted pictures [1, 2]. To fix this, people have turned to techniques like histogram equalization and adaptive filtering, but these often struggle to preserve all the delicate details and can sometimes m ake images look unnatural [11, 12].

---

## Chunk 11

=== Page 5 ===
2 possibilities for improving these images more intelligently and effectively [6].  Methods such as SRCNNs and U-Net models have shown promising results in making medical images clearer, sharper, and more detailed, which really helps doctors see better during exams [16, 18].  What's more, these models can be integrated into real-time systems, providing instant image enhancements during proce dures.  In this project, we introduce a new deep learning method that combines t he strengths of SRCNNs with a Dual-Input U-Net architecture to boost endoscopic image quality.  The SRCNN part is used to upscale and sharpen low- resolution images, while the Dual-Input U-Net refines the texture s and structures by analyzing both the original and the enlarged images together [9, 11, 12].  To make sure tha t the model retains true-to-life details and doesn’t lose i mportant features, we developed a custom loss function that blends L1 loss, MSE, and an edge-aware component [16, 21].

---

## Chunk 12

The SRCNN part is used to upscale and sharpen low- resolution images, while the Dual-Input U-Net refines the texture s and structures by analyzing both the original and the enlarged images together [9, 11, 12].   To make sure tha t the model retains true-to-life details and doesn’t lose i mportant features, we developed a custom loss function that blends L1 loss, MSE, and an edge-aware component [16, 21]. After the deep learning processing, additional post-proce ssing ste ps like sharpening, lighting correction, gamma adjustment, and noise reduction are applied to further enhance image clarity and usefulness for diagnosis [3,4,19,18].  We evaluated our approach using common image quality metrics like PSNR and SS IM which are widely used in medical image a nalysis [7, 8].  The results show that our combined method considerably improves image quality, emphasizing its strong potential for clinical use and real-time diagnostic systems [1, 10].

---

## Chunk 13

We evaluated our approach using common image quality metrics like PSNR and SS IM which are widely used in medical image a nalysis [7, 8].   The results show that our combined method considerably improves image quality, emphasizing its strong potential for clinical use and real-time diagnostic systems [1, 10]. 2 Literature Review Analyzing endoscopic images with deep learning is an exciting way to help with diagnosis; improve i mage quality; and, with any luck, lead t o better dec isions being made by doctors.  The state of the art, as far as features and classification go, has been largely set by convolutional neural networks (CNNs) [9, 11].  The favorite models these days seem to be ResNet [9] and DenseNet [11] .  They represent such a step forward in architecture that they make training networks to do what we want them to do much smoother and more efficient.  F rom here, t he big question for us seems to be: What next?  Towards what end can we push these models?

---

## Chunk 14

F rom here, t he big question for us seems to be: What next?   Towards what end can we push these models? For clinically relevant applications, the amount of labeled data available is worryingly small.  To handle this problem, we're using transfer learning (TL) and do main adaptation (DA) t o a degree that has made these approaches almost necessary at this point.  Whi le all of this is happening, image enhancement itself is also mov ing forward.  Generative adversarial networks (GANs) [19] repr esent the most advanc ed option we have at present.  They not only reduce noise in images but also sharpen them up.  Alongside everything else, depth and pose esti mation see ms to have started moving with much more intensity than it has in previous years.  Table 1.

=== Page 6 ===

---

## Chunk 15

3 ResNet Addresses Enables deep Risk of overfitting vanishing networks gradients Transfer Learning Uses pre-trained Helps t o minimize Domain similarity models reliance on affects manually labeled performance data.  GANs Image High-quality Requires careful enhancement & synthesis tuning artifact removal Self-Supervised Learns from Reduces Lower accuracy Learning unlabeled data annotation vs.  supervised dependency learning Ensemble Combines Improves Higher Learning multiple models robustness computational cost 3 Research Methodology 3. 1 Dataset Description and Preprocessing Details To test and validate our deep learning-based image enhance ment method, we used five publ icly available endoscopic datasets [3, 4, 17, 22, 25].  These datasets include images representing a wide range of gastrointestinal issues like polyps, ulcers, and inflammation, which helps ensure our model’s performance is both reliable and relevant to real clinical scenarios.

---

## Chunk 16

1 Dataset Description and Preprocessing Details To test and validate our deep learning-based image enhance ment method, we used five publ icly available endoscopic datasets [3, 4, 17, 22, 25].   These datasets include images representing a wide range of gastrointestinal issues like polyps, ulcers, and inflammation, which helps ensure our model’s performance is both reliable and relevant to real clinical scenarios. They also feature different imaging condit ions such as poor lighting, motion blur, and variations in anatomy, which is essential for making sure our appr oach works well acr oss different situations [2, 5, 19].  Plus, since these datasets are commonly used in recent research on endoscopic image enhancement and classification, it makes it easier to compare our results with the best existing methods [16, 20].

---

## Chunk 17

They also feature different imaging condit ions such as poor lighting, motion blur, and variations in anatomy, which is essential for making sure our appr oach works well acr oss different situations [2, 5, 19].   Plus, since these datasets are commonly used in recent research on endoscopic image enhancement and classification, it makes it easier to compare our results with the best existing methods [16, 20]. We’ve summarized key details such as resolution, dataset si ze, and types of annotations in Table 2, while Figure 1 displays sample images tha t emphasize the variety of pathologies and visual challenges faced in endoscopic imaging.  Table 1.  SUMMARY OF ENDOSCOPIC IMAGE DATASETS Name Size Kvasir SEG [22] 44. 1 MB Kvasir - Computer-Aided Gastrointestinal Disease Detection [22] 2.

=== Page 7 ===

---

## Chunk 18

4 Fig.  1.  Sample images from th e datasets display a variety of gastrointestinal conditions along with associated artifacts.  All images are resized t o 299 ∗ 299 Pixels and standard normalization techniques are applied during pre processing to ensure consistency before feeding them i nto the model.  3. 2 Deep Learning-Based Enhancement The enhancement framework i s buil t around a two-step deep learning setup.  It starts with an S RCNN to boost the resolution, followed by a Dua l-Input U- Net that fine- tunes the structure and texture details [9, 11].  The segre gation between operations establishes a process for the system to improve low-quality endoscopic images through SRCNN because of its proven success in medical image re solution enhancement tasks [6, 12].  During Dual- Input U-Net processing, both the original image and the higher-resolution version are analyzed together to enhance anatomical details.

---

## Chunk 19

The segre gation between operations establishes a process for the system to improve low-quality endoscopic images through SRCNN because of its proven success in medical image re solution enhancement tasks [6, 12].   During Dual- Input U-Net processing, both the original image and the higher-resolution version are analyzed together to enhance anatomical details. This approach uses mul ti-scale features and combines information from both inputs t o improve clarity [16].  The fra mework integrates m ultiple st ages to maintain detailed image structures since it reduces typical distortions that conventional endoscopic imaging encounters [3, 4, 5].  SRCNN SRCNN model is an appli cation of the proposed model which is used to create high frequencies using low frequencies and low resolutions as the inputs [9].

---

## Chunk 20

The fra mework integrates m ultiple st ages to maintain detailed image structures since it reduces typical distortions that conventional endoscopic imaging encounters [3, 4, 5].   SRCNN SRCNN model is an appli cation of the proposed model which is used to create high frequencies using low frequencies and low resolutions as the inputs [9]. The network architecture consists of 3 subsequent convolutional layers, which are developed by the main role in the super-resolution procedure, as shown in the Figure 2:  Feature Extraction Layer: To extract l ow level features of the input image, capture important patterns and structural clues, a 9*9 Convolutional layer is applied.

---

## Chunk 21

=== Page 8 ===
5  Non-linear Mapping Layer: The layer uses 5*5 Convolution to map the extracted features to different aspect of a feature space bec ause t he network i s able t o learn complex representations.   Reconstruction Layer: The second 5*5 convolution feature maps ar e reassembled into the high-resolution picture using the reconstruction layer so that the fine textures of the picture can be reconstructed and the spatial details saved.  This effective yet simple architecture makes the model learn the end to end mapping between the low and hi gh resolutions images in an end-to-end manner which has been highly effective in many medical imaging applications [6, 12].  Fig.  1.  The architecture of the SRCNN model is used for super-resolution enhancement.  Dual Input U-Net The middle result of the SRCNN is then combined with the original low-resolution image and inserted into a U-Net model to be enhanced in terms of structural integrity further [9, 16].

---

## Chunk 22

The architecture of the SRCNN model is used for super-resolution enhancement.   Dual Input U-Net The middle result of the SRCNN is then combined with the original low-resolution image and inserted into a U-Net model to be enhanced in terms of structural integrity further [9, 16]. Differently than in Figure 1, the U- Net architec ture has three main sections as shown in Figure 3:  Encoder : This part catches global context details implemented using a succession of convolutional layers and m ax pooling operations, thus, squeezing spatial distribution with maintaining of the necessary information.   Bottleneck : This section of the network is interested in deriving high level, general features that it can use to identify and lear n more complicated structures out of their combined input.

---

## Chunk 23

Differently than in Figure 1, the U- Net architec ture has three main sections as shown in Figure 3:  Encoder : This part catches global context details implemented using a succession of convolutional layers and m ax pooling operations, thus, squeezing spatial distribution with maintaining of the necessary information.    Bottleneck : This section of the network is interested in deriving high level, general features that it can use to identify and lear n more complicated structures out of their combined input.  Decoder: The decoder combines upsampling methods with the skip connections t o the respective encoder layers, which allows not only restoring fine-grained deta ils accurately but also improving the quality of the entire image in general.

=== Page 9 ===

---

## Chunk 24

6 The design i s effective because it has both the strengths of SRCNN and U- Net, and it ensures that the images become sharper yet maintains the details that are important in terms of accurate clinical evaluation [6, 12].  Fig.  1.  Skip connection and encoder-decoder based Dual-Input U-Net structure..  In order to maximize training and perfect the perceptual quality, as well as the structural ac curacy of the improved endoscopic images, a composite loss is ut ilized.  This loss combi nes many components which have a contribution to various attributes of image quality:  L1 Loss: Promotes pixel based similar ity between the predicted image and the ground truth image by minimizing t he absolute difference in intensities betwe en the two images.   MSE Loss: Emphasizes on decreasing the errors of the overall reconstruction at the cost of bigger deviations and encourages smoother solutions.

---

## Chunk 25

This loss combi nes many components which have a contribution to various attributes of image quality:  L1 Loss: Promotes pixel based similar ity between the predicted image and the ground truth image by minimizing t he absolute difference in intensities betwe en the two images.    MSE Loss: Emphasizes on decreasing the errors of the overall reconstruction at the cost of bigger deviations and encourages smoother solutions. Edge Loss: The algorithm is based on gradient di fferences computed using Sobel filter to strengthen edges, so as to preserve the visibility of ti ny anatomical borders and underlying building blocks [11, 16].  The loss function that consists of the total loss is denoted by: L total = α L 1 + β L MSE + γ L edge ( SEQ equation ¿ ¿ MERGEF ORMA T 1 ) where α , β , γ are weighting factors.

---

## Chunk 26

Edge Loss: The algorithm is based on gradient di fferences computed using Sobel filter to strengthen edges, so as to preserve the visibility of ti ny anatomical borders and underlying building blocks [11, 16].   The loss function that consists of the total loss is denoted by: L total = α L 1 + β L MSE + γ L edge ( SEQ equation ¿ ¿ MERGEF ORMA T 1 ) where α , β , γ are weighting factors. Post-Processing Techniques To improve more the image clarity, contrast, and the diagnostic quality, the suggested post processing techniques are carried out in Table 3 as under: Table 1.

=== Page 10 ===

---

## Chunk 27

7 Illumination Enhancement Contrast adjustment per formed using CLAHE in order to LAB color space.  Gamma Correction Gamma t ransformation is non-linear adjustment of brightness Denoising Effective reduc tion of noise by Non- Local Means filter Fig.  1.  Comparison of raw images and enhanced images after post-processing.  Enhancement Pipeline The entire process of enhancement is structured into an effective pipeline, as it is shown in Figure 5.  Each of the stages will aim at the gradual enhanced quality of imaging with the increased diagnostic efficiency: 1.  Preprocessing: All the dataset images have been loaded and resize d to standard resolution of 299*299 P ixels.  Normalization is subsequently performed to scale all the input in a consistent way through the pipeline.  2.  Super-Resolution: The SRCNN model undertakes the first stage of improvement of the image in relieving a high-frequency detail in images and enhancing resolution.  3.

---

## Chunk 28

Super-Resolution: The SRCNN model undertakes the first stage of improvement of the image in relieving a high-frequency detail in images and enhancing resolution.   3. Structural Refinement: A Dual Input U-Ne t takes in the original low-resolution image and the image enhanced by SRCNN and improves structural textures, and takes care of the anatomical integrity.  4.  Post-Processing: There is a series of i mproving t ricks that are used such as sharpening, correction of light and gamma.  Such actions fur ther supplement the clarity of the images and their visual quality.  5.  Evaluation: The evaluation of the image quality is based on PSNR as well as SSIM as typical performance metrics used to evaluate the eff ectiveness of the enhancement output [6, 12].

=== Page 11 ===

---

## Chunk 29

8 Fig.  1.  Deep learning based endoscopic image enhancement frame work flowchart The deep learning-based enhancement framework comprises three essential components: SRCNN for resolution enhancement [9], Dual-Input U-Net for structural refinement [16], and various post-processing methods that generate high-quality endoscopic images free from ar tifacts.  These combined elements lead to noticeable improvements i n image quality, including sharper detail, higher resolution, and clearer features.  Tests have shown important gains in both PSNR and SSIM, which confirms that this approach is effective [6, 12].  Overall, t his method marks an important step for ward in medical im age enhance ment, showing strong potential for use in real-time diagnostics and clinical workflows.  3.

---

## Chunk 30

Overall, t his method marks an important step for ward in medical im age enhance ment, showing strong potential for use in real-time diagnostics and clinical workflows.   3.3 Comparative Novelty Analysis Our proposed enhance ment fra mework introduces an innovative approach by combining a Super-Resolution Convolutional Neural Network (SRCNN) with a Dual- Input U-Net to enhance both the resolution and structural acc uracy of endoscopic images.  While i ndividually, SRCNN and U- Net architectures have been used in medical image enhancement, their sequential integration—first employing SRCNN to boost spatial resolution, followed by a dual-input U-Net for structural refinement— represents a new advancement in the field.  Traditional enhancement methods usually rely on single-input models, which ca n often result in the loss of fine anatomical details or important contextual information.

---

## Chunk 31

While i ndividually, SRCNN and U- Net architectures have been used in medical image enhancement, their sequential integration—first employing SRCNN to boost spatial resolution, followed by a dual-input U-Net for structural refinement— represents a new advancement in the field.   Traditional enhancement methods usually rely on single-input models, which ca n often result in the loss of fine anatomical details or important contextual information. In contrast, our Dual-Input U- Net is specifically designed to process both the original low-resolution image and the super- resolved image simultaneously.  This dual-path setup allows the network to extract high-frequency textures from the SRCNN output while prese rving the contextual integrity of the original image.  As a result, our model delivers improved perceptual quality and preserves fine details acr oss complex regions such as mucosal folds, shadows, and areas affected by motion blur.

---

## Chunk 32

This dual-path setup allows the network to extract high-frequency textures from the SRCNN output while prese rving the contextual integrity of the original image.   As a result, our model delivers improved perceptual quality and preserves fine details acr oss complex regions such as mucosal folds, shadows, and areas affected by motion blur. Below, we compare our hybrid approach with two recent state-of-the-art methods in the table 4.  Table 4.  CONTRASTS OUR DESIGN AGAINST RECENT STATE-OF-THE-ART MODELS Architecture PSNR/SSIM Remarks Paper Name Overview This Paper SRCNN + Dual- Combines resolution Input U-Net (two- enhancement and 38. 77 / 0. 98 stage hybrid structural refinement; pipeline) flexible & extendable.  A Lightweight Lightweight CNN 39.

=== Page 12 ===

---

## Chunk 33

9 CNN for Detail focused on low-light Enhancement and with IGEB and WCE images but less Color Correction of 0. 9792 TCRB blocks extensible or Low-Light Capsule interpretable.  Endoscopy [21] Global and Local Curve-based End-to-end model Enhancement of global module + 27. 32 / designed for uneven Low-light dual attention + 0. 8349 illumination correction Endoscopic Images denoising in low-light endoscopy.  [16] While our approach slightly trails method proposed by Shuocheng Wang et al [21] in PSNR, it surpasses in structural similarity (SSIM) and offers impressive flexibility thanks to its modular design.  Besides, our pipeline allows for customized post- processing steps tha t improve clinical interpretability, something end-to-end models often struggle t o provide.  This hybrid strategy provides a good middle ground, balancing clarity, resolution, and adaptability, making it especially suitable for practical medical imaging applications.

---

## Chunk 34

Besides, our pipeline allows for customized post- processing steps tha t improve clinical interpretability, something end-to-end models often struggle t o provide.   This hybrid strategy provides a good middle ground, balancing clarity, resolution, and adaptability, making it especially suitable for practical medical imaging applications. 4 Results The evaluation of the deep learning based enhancement fra mework unfolds in this section.  The assessment of the model focuses on quantitative evaluation alongside qualitative visual analysis before performing an extensive analysis of positive points, drawbacks, and possible enhancements.  4. 1 Evaluation Metrics The performance evaluation of the enhancing t echnique relies on two established image quality assessment metrics.  Image quality assessment begins wi th the PSNR, which measures the relation between signal power capa city and noise-related power that degrades image quality.

---

## Chunk 35

1 Evaluation Metrics The performance evaluation of the enhancing t echnique relies on two established image quality assessment metrics.   Image quality assessment begins wi th the PSNR, which measures the relation between signal power capa city and noise-related power that degrades image quality. Higher PSNR values mean the image quality improves because there's less distortion present.  Mathematically, PSNR is defined as: 2 MA X I (1) PSNR = 10 log 10 ( ) MSE where MA X I The maximum possible pixel value of the image (e. g. , 255 for 8- bit images), and MSE denotes the Mean Squared Error betwee n the original and the enhanced images [24, 27].

=== Page 13 ===

---

## Chunk 36

10 The SSIM is a commonly used m etric for assessing image quality.  It compares two images based on how similar they look to the human eye, focusing on aspects like brightness, contrast, and the overall structure of the details.  In contrast to PSNR, which focuses on individual pixels, SSIM offers a more comprehensive evaluation of image fidelity by integrating principles of human visual perception.  SSIM is mathematically defined as: ( 2 μ x μ y + C 1 ) ( 2 σ xy + C 2 ) SSIM ( x , y ) = (1) 2 2 2 2 + μ y + C 1 ) ( σ x + σ y + C 2 ) ( μ x where μx , μy Represent mean intensities, σx , σy Denote variances, and σxy Indicates the cova riance.  C 1 and C 2 These are small constants that stabilize the division to avoid instabilities when denominators are close to zero [24].  Through the SRCNN model, image resolution showed significant enhancement resulting in PSNR of 38. 77 dB and the SSIM of 0. 98 which represents a high similarity to the original input and good structural maintenance.

---

## Chunk 37

77 dB and the SSIM of 0.  98 which represents a high similarity to the original input and good structural maintenance. Then the Dual-Input U-Net plays the role of further sharpening fine details and continuity of textures.  Even though this st age produces a worse PSNR of 31. 18 dB and SSIM of 0. 89, the perceptual quality and anatomical nature of images are evidently better than those produced by the previous stage.  After the last step of post-processing, which contains contrast enhancement, sharpening, and removal of noise, the metrics drop to PSNR of 23. 97 dB and SSIM of 0. 77.  Th is is mostly because of the changes made in bold pixels but as a whole t he image is easier to interpr et and diagnose.  It demonst rates that the method can be helpful in clinical practice.  4. 2 Visual Comparison In addition to the nu merical metrics, perceptual image quality plays a critical role in medical diagnosis, as it directly influences clini cians' ability to interpret endoscopic images.

---

## Chunk 38

4.  2 Visual Comparison In addition to the nu merical metrics, perceptual image quality plays a critical role in medical diagnosis, as it directly influences clini cians' ability to interpret endoscopic images. To assess t he effectiveness of the proposed enhancement methods, a visual comparison is made between the original endoscopic images and those enhanced by the SRCNN and Dual-Input U-Net.  This comparison demonstrates clear improvements in image sharpness, contrast, and the smoothness of textures, as illustrated in Figure 6.  These visual evaluations are essential for gauging how the i mprovements actually influence diagnostic processes in real-world settings.

=== Page 14 ===

---

## Chunk 39

11 Fig.  1.  Comparative visualization of endoscopic images before and aft er enhancement using SRCNN and Dual-Input U-Net.  Observations from Visual Comparisons:  Raw Endoscopic Images: The original input images exhibit noticeable blur, low contrast, and considerable noise, all of which hinder accurate clinical interpretation and reduce diagnostic reliability.   Enhanced Image: The image main tains clarity in structura l details, smooth transitions i n texture, and strong contrast.  Small anatomical features become more noticeable, thanks to the improved resolution from upscaling, which also brings sharper edge definition.  However, residual noise and mild blurriness remain, especially in complex regions, which will be tackled by post-processing.  4. 3 Effect of Post-Processing T echniques Beyond the initial enhancements powered by deep learning, we also apply a range of post-processing techniques to further sharpen image quality.

---

## Chunk 40

4.  3 Effect of Post-Processing T echniques Beyond the initial enhancements powered by deep learning, we also apply a range of post-processing techniques to further sharpen image quality. These steps help improve contrast, correct uneven lighting, and reduce noise, which finally makes the images clearer and easier to interpret, especially for medical diagnosis purposes.  Table 1.  EXAMPLES OF POST-PROCESSING IMPACT ON IMAGE QUALITY Post-Processing Technique PSNR (dB) SSIM ↑ Effect on Image ↑ Sharpening + 0. 8 + 0. 03 Enhances fine edges and details CLAHE (Illumination + 1. 2 + 0. 04 Corrects uneven Correction) lighting Gamma Correction + 0. 7 + 0.

---

## Chunk 41

=== Page 15 ===
12 non-linearly Fig.  1.  Comparative analysis of different post-processing techniques on endoscopic images.  Key Findings from selected Post-Processing Analysis:  CLAHE and White Balancing: These methods greatly enha nce bot h the bright ness and color accuracy of images, resulting in vi suals that look more natural and provide better clarity for diagnostic purposes.   Denoising Techniques: Noise is reduced so that the important parts of the anatomy stay clear, m aking sure no critical details get washed out when the image is improved.   Bilateral Filtering: This approach enhances image smoothness and texture while maintaining sharp edge definition, balancing clarity and realism.  5 Discussion Advantages of the Proposed Approach.   Combining greatly enhances image resolution, S RCNN with Dual-Input U-Net reduces noise, and preserves structural details, surpassing traditional methods.

---

## Chunk 42

5 Discussion Advantages of the Proposed Approach.    Combining greatly enhances image resolution, S RCNN with Dual-Input U-Net reduces noise, and preserves structural details, surpassing traditional methods.  Edge-preserving enh ancement uses a special loss function that helps keep important boundaries in medical images clear.   Strong Artifacts Resistance: This technique off ers excellent resilience against common challenges faced in endosc opy, such as motion-induced blur, ref lections from shiny surfaces, and inconsistent lighting conditions.   Clinical Effectiveness: Improving t exture clarity and minimizing image noise enhances the precision in detec ting polyps, assists better segme ntation of lesions, and boosts overall confidence in diagnoses during gastrointestinal procedures.

=== Page 16 ===

---

## Chunk 43

13 Table 6.  PROPOSED ARCITECTURE’S COMPUTATIONAL PROFILE Parameters A vg Inference Time Component Memory (approx. ) (M) (ms) SRCNN ~ 0. 58 ~ 120 MB ~ 8 Dual-Input U-Net ~ 3. 5 ~ 480 MB ~ 19 Post-Processing - ~ 50 MB ~ 12 Stack Total - ~ 650 MB ~ 39 These findings confirm that the model is both lightweight and fast, and is suitable for integration into clinical-grade image enhancement systems.  Limitations  Dataset-Specific Training: The model has been primar ily fine-tuned using specialized endoscopic datasets.  This focus may restrict its applicability to other types of medical imaging or varied clinical environments.   Over-Smoothing Risk: Intensive post-processing methods might unintentionally result in the l oss of delicate textural details, which could affect the precise evaluation of pathological characteristics.

---

## Chunk 44

This focus may restrict its applicability to other types of medical imaging or varied clinical environments.    Over-Smoothing Risk: Intensive post-processing methods might unintentionally result in the l oss of delicate textural details, which could affect the precise evaluation of pathological characteristics. 6 Conclusion This study aims to develop an advanced image enhancement framew ork based on deep learning techniques, specifically personalized to improve the quality of endoscopic images.  The prim ary focus was on addressing common challenges such as low resolution, high noise levels, and poor contrast, with the goal of producing clearer images that are more useful for medical diagnosis.  The proposed dual-stage approach combines an SRCNN with a Dual-Input U-Net, which considerably improves image sharpness, contrast, and the preservation of important structural details [9, 16].

---

## Chunk 45

The prim ary focus was on addressing common challenges such as low resolution, high noise levels, and poor contrast, with the goal of producing clearer images that are more useful for medical diagnosis.   The proposed dual-stage approach combines an SRCNN with a Dual-Input U-Net, which considerably improves image sharpness, contrast, and the preservation of important structural details [9, 16]. Quantitative measures like PSNR and SSIM clearly indicate significant enhancements in the quality of the image [24, 27] and thi s highlights the ability of the m odel to come up with results which are worthy of being used in the diagnosis.  Furthermore post processing abilities to enhance the clarity of an image such as sharpening, gamma correc tion, lighting correction, denoising and white balancing are also implementable.

---

## Chunk 46

=== Page 17 ===
14 to ensure that all images are rendered with precision that is reliable to make t he diagnosis.  7 Future Scope Improving Generalization Across Datasets  Diverse Dataset Integration: Gathering information from multiple medical facilities helps cover a wider range of imaging scenarios and clinical variations.   Domain Adaptation Techniques: Investigating techniques t hat can connect images captured from various endoscope manufacturers and environments, t hus improving the model’s resilience.   Self-Supervised and Contrastive Learning: Using these approaches helps improve the model’s ability to generalize without relying extensively on large labelled datasets, allowing it to p ick up on fundamental features directly from unlabelled images.  Integration with AI-Assisted Diagnosis  Automated Detection: This process focuses on creating object detection algorithms that ca n automatically identi fy ar tifacts, which in turn supports earlier and more precise diagnoses.

---

## Chunk 47

 Self-Supervised and Contrastive Learning: Using these approaches helps improve the model’s ability to generalize without relying extensively on large labelled datasets, allowing it to p ick up on fundamental features directly from unlabelled images.   Integration with AI-Assisted Diagnosis  Automated Detection: This process focuses on creating object detection algorithms that ca n automatically identi fy ar tifacts, which in turn supports earlier and more precise diagnoses.  Segmentation for Precise Localization: Using advanced deep learning segmentation networks, we ca n precisely identify specific area s of interest.  This allows for more targeted and efficient clinical assessments, improving overall diagnostic accuracy.

---

## Chunk 48

 Segmentation for Precise Localization: Using advanced deep learning segmentation networks, we ca n precisely identify specific area s of interest.   This allows for more targeted and efficient clinical assessments, improving overall diagnostic accuracy.  AI-Driven Decision Support Systems: Our AI-powered decision support systems operate in real time, using advanced imaging and diagnostic algorithms to give clinicians actionable insights during endoscopic procedures, enhancing decision- making and patient outcomes.

---

## Chunk 49

=== Page 18 ===
AI Content 0% Text Coverage Words A I Text 0% 0 Human Text 100% 3,876 Excluded Omitted Words 9 About A I Detection Our A I Detector is the only enterprise-level solution that can verify if the content was written by a human or generated by A I , including source code and text that has been plagiarized or modified.  Learn more AI Text Human Text A body of text that has been generated or altered by A I technology.  Any text that has been fully written by a human and has not been Learn more altered or generated by A I .  Learn more Copyleaks A I Detector Effectiveness Credible data at scale, coupled with machine learning and widespread adoption, allows us to continually refine and improve our ability to understand complex text patterns, resulting in over 99% accuracy—far higher than any other A I detector—and improving daily.

---

## Chunk 50

Any text that has been fully written by a human and has not been Learn more altered or generated by A I .   Learn more Copyleaks A I Detector Effectiveness Credible data at scale, coupled with machine learning and widespread adoption, allows us to continually refine and improve our ability to understand complex text patterns, resulting in over 99% accuracy—far higher than any other A I detector—and improving daily. Learn more I deal Text Length The higher the character count, the easier for our technology to determine irregular patterns, which results in a higher confidence rating for A I detection.  Learn more Reasons I t Might Be A I When You Think I t's Not The A I Detector can detect a variety of A I -generated text, including tools that use A I technology to paraphrase content, auto-complete sentences, and more.  Learn more User A I Alert History Historical data of how many times a user has been flagged for potentially having A I text within their content.

---

## Chunk 51

Learn more Reasons I t Might Be A I When You Think I t's Not The A I Detector can detect a variety of A I -generated text, including tools that use A I technology to paraphrase content, auto-complete sentences, and more.   Learn more User A I Alert History Historical data of how many times a user has been flagged for potentially having A I text within their content. Learn more A I Logic The number of times a phrase was found more frequently in A I vs human text is shown according to low, medium, and high frequency.

---

