{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ba46c76",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --quiet langchain langchain-community langchain-google-genai faiss-cpu google-generativeai python-dotenv PyPDF2 docx2txt pandas openpyxl pillow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06c2c2f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install easyocr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f9f55138",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Google API key loaded: AIzaSyBARx ********\n",
      "‚úÖ Gemini connection successful! Model responded.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import google.generativeai as genai\n",
    "\n",
    "# Load environment variables from the .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Check and print partial key for confirmation\n",
    "api_key = os.getenv(\"GOOGLE_API_KEY\")\n",
    "\n",
    "if not api_key:\n",
    "    raise ValueError(\"‚ùå GOOGLE_API_KEY not found. Please check your .env file.\")\n",
    "else:\n",
    "    print(\"‚úÖ Google API key loaded:\", api_key[:10], \"********\")\n",
    "\n",
    "# Configure Google Generative AI client\n",
    "genai.configure(api_key=api_key)\n",
    "\n",
    "# Quick test to verify API connection\n",
    "try:\n",
    "    test_model = genai.GenerativeModel(\"gemini-2.0-flash\")\n",
    "    response = test_model.generate_content(\"Test connection to Gemini API.\")\n",
    "    print(\"‚úÖ Gemini connection successful! Model responded.\")\n",
    "except Exception as e:\n",
    "    print(\"‚ùå Gemini API connection failed:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3ac8dd68",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using CPU. Note: This module is much faster with a GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÑ Loading: input\\20251014-174708_Top AC Repa.csv\n",
      "üìÑ Loading: input\\AI and Plag Report_chunks.md\n",
      "üìÑ Loading: input\\Embeddings and Vector Search.jpeg\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import os\n",
    "import easyocr\n",
    "\n",
    "# LangChain loaders\n",
    "from langchain_community.document_loaders import (\n",
    "    TextLoader, PyPDFLoader, JSONLoader, Docx2txtLoader, CSVLoader, UnstructuredExcelLoader\n",
    ")\n",
    "\n",
    "# Compatible import for Document\n",
    "try:\n",
    "    from langchain_core.documents import Document\n",
    "except ImportError:\n",
    "    from langchain.schema import Document\n",
    "\n",
    "# Initialize EasyOCR (English only; add other languages if needed)\n",
    "ocr_reader = easyocr.Reader(['en'], gpu=False)\n",
    "\n",
    "# Folder path for your data\n",
    "input_folder = \"input\"\n",
    "if not os.path.exists(input_folder):\n",
    "    raise FileNotFoundError(f\"‚ùå Folder not found: {input_folder}\")\n",
    "\n",
    "# Supported file extensions\n",
    "supported_exts = [\".txt\", \".md\", \".pdf\", \".docx\", \".json\", \".csv\", \".xlsx\", \".xls\", \".jpeg\", \".jpg\", \".png\"]\n",
    "\n",
    "# Storage for all documents\n",
    "all_docs = []\n",
    "\n",
    "# OCR function using EasyOCR\n",
    "def load_image_text(image_path):\n",
    "    \"\"\"Extract text from images using EasyOCR.\"\"\"\n",
    "    try:\n",
    "        results = ocr_reader.readtext(image_path, detail=0)\n",
    "        text = \"\\n\".join(results)\n",
    "        return text.strip()\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è EasyOCR failed for {image_path}: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "# Iterate through files in input folder\n",
    "for filename in os.listdir(input_folder):\n",
    "    filepath = os.path.join(input_folder, filename)\n",
    "    ext = os.path.splitext(filename)[-1].lower()\n",
    "\n",
    "    if ext not in supported_exts:\n",
    "        print(f\"‚è© Skipping unsupported file: {filename}\")\n",
    "        continue\n",
    "\n",
    "    print(f\"üìÑ Loading: {filepath}\")\n",
    "    try:\n",
    "        if ext in [\".txt\", \".md\"]:\n",
    "            loader = TextLoader(filepath)\n",
    "            docs = loader.load()\n",
    "        elif ext == \".pdf\":\n",
    "            loader = PyPDFLoader(filepath)\n",
    "            docs = loader.load()\n",
    "        elif ext == \".docx\":\n",
    "            loader = Docx2txtLoader(filepath)\n",
    "            docs = loader.load()\n",
    "        elif ext == \".json\":\n",
    "            try:\n",
    "                loader = JSONLoader(filepath, jq_schema=\".\", text_content=False)\n",
    "                docs = loader.load()\n",
    "            except Exception:\n",
    "                import json\n",
    "                with open(filepath, \"r\", encoding=\"utf-8\") as f:\n",
    "                    data = json.load(f)\n",
    "                docs = [Document(page_content=str(data), metadata={\"source\": filename})]\n",
    "        elif ext == \".csv\":\n",
    "            loader = CSVLoader(filepath)\n",
    "            docs = loader.load()\n",
    "        elif ext in [\".xlsx\", \".xls\"]:\n",
    "            loader = UnstructuredExcelLoader(filepath)\n",
    "            docs = loader.load()\n",
    "        elif ext in [\".jpeg\", \".jpg\", \".png\"]:\n",
    "            text = load_image_text(filepath)\n",
    "            docs = [Document(page_content=text, metadata={\"source\": filename})] if text else []\n",
    "        else:\n",
    "            docs = []\n",
    "\n",
    "        all_docs.extend(docs)\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Failed to load {filename}: {e}\")\n",
    "\n",
    "print(f\"\\n‚úÖ Total documents loaded: {len(all_docs)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca4e06a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Initialize the text splitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,      # ~1000 characters per chunk\n",
    "    chunk_overlap=200,    # overlap preserves context between chunks\n",
    "    length_function=len,\n",
    ")\n",
    "\n",
    "# Split all loaded documents into chunks\n",
    "chunks = text_splitter.split_documents(all_docs)\n",
    "\n",
    "print(f\"‚úÖ Chunks created: {len(chunks)} from {len(all_docs)} source documents.\\n\")\n",
    "\n",
    "# Preview first chunk\n",
    "if chunks:\n",
    "    print(\"Example chunk preview:\\n\")\n",
    "    print(chunks[0].page_content[:500] + \"...\")\n",
    "else:\n",
    "    print(\"No chunks created.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26e33e0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "# Local embedding model (no API required)\n",
    "embedding_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "# Build FAISS index\n",
    "vectorstore = FAISS.from_documents(chunks, embedding=embedding_model)\n",
    "\n",
    "# Create retriever\n",
    "retriever = vectorstore.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 4})\n",
    "\n",
    "print(\"‚úÖ FAISS vector store created successfully using HuggingFace embeddings.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdcfb67b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
